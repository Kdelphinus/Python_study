{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training Neural Networks\n",
    "2. Convolution Neural Network\n",
    "3. CNN - Case Study\n",
    "4. Recurrent Neural Networks\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Training Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Sigmoid activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid](_image/sigmoid.png)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "앞서 배웠듯이 0 ~ 1 사이 값을 가지는 함수입니다. 그러나 Neural Networks에서 사용하기엔 많은 문제점이 있습니다.\n",
    "\n",
    "- 기울기 소실\n",
    "- output의 중간값이 0이 아니다.\n",
    "- exp()의 연산이 너무 무겁다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Softmax activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_i = \\frac{e^{z_i}}{\\sum_{j = 1}^k e^{z_j}}$$\n",
    "\n",
    "sigmoid가 두 개의 값으로 분류한다면 softmax는 여러 가지 값으로 분류한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) tanh activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tanh](_image/tanh.png)\n",
    "\n",
    "$$tanh(x) = 2 \\times sigmoid(x) - 1$$\n",
    "\n",
    "-1 ~ 1의 범위를 갖고 있습니다. sigmoid의 문제점 중 하나인 output의 중간값을 0으로 만들었습니다. 그러나 기울기 소실 문제는 여전히 남아있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) ReLU(Rectified Linear Unit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU](_image/ReLU.png)\n",
    "\n",
    "$$computes \\; f(x) = max(0, x)$$\n",
    "\n",
    "학습이 굉장히 빠르고 기울기 소실 문제가 사라집니다. output의 중간값이 0은 아니지만 많이 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid나 tanh에서 중요한 것 중 하나는 기울기가 살아있는 범위에 input 데이터가 들어가야 유의미한 값이 나온다는 것입니다. 그걸 위해 사용하는 방법이 **Batch Normalization** 입니다. 이를 통해 기울기 소실 문제를 해결할 수 있습니다. \n",
    "\n",
    "$$\\hat{x}^{(k)} = \\frac{x^{(k)} - \\text{E}[x^{(k)}]}{\\sqrt{\\text{Var}[x^{(k)}]}}$$\n",
    "\n",
    "위 식을 통해 평균 0, 분산 1의 input data가 만들어집니다. \n",
    "\n",
    "그 후, Neural Network가 데이터에 말맞게 평균과 분산을 조절합니다.\n",
    "\n",
    "$$y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}$$\n",
    "\n",
    "이를 통해 $\\gamma$는 표준편차를, $\\beta$는 평균을 학습시킨다. 이렇게 $\\gamma , \\beta$의 최적값을 구하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimization을 위한 방법은 여러가지가 있습니다. 왜냐하면 gradient descent를 그대로 사용하면 변동폭이 너무 크게 일어나기 때문입니다. 이제 하나씩 살펴보겠습니다.\n",
    "\n",
    "여러 방식들이 어떻게 이루어지는지 gif로 보려면 다음 링크로 접속하여 확인하면 됩니다.\n",
    "http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 5 # input data\n",
    "dx = 2 # gradient\n",
    "learning_rate = 0.1 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Gradient Descent(SGD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sgd](_image/sgd.png)\n",
    "\n",
    "일반적인 방법은 현재 자신의 위치에서 측정한 경사에 따라 움직입니다. 그렇기에 위 그림처럼 진동이 엄청나게 일어납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla gradient descent update\n",
    "x -= learning_rate * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Momentum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![momentum](_image/momentum.png)\n",
    "\n",
    "Momentum은 현재 위치에서 측정한 기울기만 사용하지 않습니다. 전에 지나온 기울기들을 계속해서 합산하여 사용합니다. 그렇기 때문에 진동이 큰 방향은 진동 크기를 줄이고 맞는 방향은 더욱 빠르게 가도록 해줍니다. \n",
    "\n",
    "기울기를 합산할 때, 예전 기울기의 영향력을 줄이기 위해서 0.8 등의 공비를 계속해서 곱해줍니다. \n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1000/1*X9SaxFM6_sBOAMY9TaGsKw.png\">\n",
    "\n",
    "이름이 momentum이듯, 관성처럼 local minimum에 머물지 않고 global minimum으로 가도록 만들어줍니다.\n",
    "\n",
    "이를 코드로 보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum update\n",
    "mu = 0.8 # 공비\n",
    "v = 0\n",
    "\n",
    "v = mu * v - learning_rate * dx\n",
    "x += v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Adagrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기들을 합산할 때, 기울기의 크기가 많이 차이나지 않도록 기울기를 제곱해서 합을 구한다음 루트를 취하여 기울기에 나눠줍니다. 코드를 통해 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad update\n",
    "cache = 0\n",
    "\n",
    "cache += dx**2\n",
    "x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 1e-7을 더해주는 이유는 cache가 너무 작아져서 0이 되어 error가 나는 것을 방지하기 위해서입니다.\n",
    "\n",
    "Adagrad는 방향을 잘 찾아가지만 목표에 도달할수록 나누는 값이 너무 커집니다. 그렇기에 한 번의 가는 거리가 점점 작아지고 속도가 느려지게 됩니다.\n",
    "\n",
    "그렇기에 Adagrad를 직접 사용하지 않고 이를 활용하는 방법들을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) RMSProp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad에서 decay_rate를추가하여 오래된 기울기들의 영향력을 제거하는 방식을 도입한 것이 RMSProp입니다. 코드로 구현하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rate = 0.1\n",
    "\n",
    "# RMSProp update\n",
    "cache += decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp와 Momentum 방식을 합쳐서 사용하는 방법이 Adam입니다. 기본적으로 beta1 = 0.9, beta2 = 0.999, eps = 1e-8을 추천됩니다. 코드로 나타내면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam upgrade\n",
    "m, v = 0, 0\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "\n",
    "m = beta1 * m + (1 - beta1) * dx\n",
    "v = beta2 * v + (1 - beta2) * (dx**2)\n",
    "x -= learning_rate * m / (np.sqrt(v) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate는 gradient descent에서 움직이는 보폭 비율이라고 생각하면 됩니다. 그렇기에 learning rate가 크면 크게크게 학습하고 작으면 차근차근 학습하게 됩니다. learning rate의 크기에 따라 학습 정도를 살펴보면 다음과 같습니다.\n",
    "\n",
    "![learning_rate](_image/learning_rate.png)\n",
    "\n",
    "하나씩 살펴보면 먼저 작은 값을 가지면 학습속도가 매우 느립니다. 만약 큰 값을 가진다면 학습속도가 빠르지만 어느 순간부터 학습이 진행되지 않습니다. 왜냐하면 현재 위치와 목표점의 거리보다 보폭이 더 크기 때문입니다. 그리고 만약 매우 큰 값을 가진다면 학습이 될 수 있지만 금방 발산해버립니다.\n",
    "\n",
    "그렇기 때문에 epoch가 진행될수록 learning rate를 줄이는 방법을 사용합니다.\n",
    "\n",
    "\n",
    "![learning_rate2](_image/learning_rate2.png)\n",
    "\n",
    "위 그림처럼 loss가 줄지 않을 때마다 learning rate를 줄여서 학습을 진행합니다. \n",
    "\n",
    "learning rate를 늘리거나 줄일 땐, 0.1, 1, 10, 100,... 등 10배씩 키우거나 줄이는 것이 일반적입니다. 또는 $\\sqrt{10} \\sim 3$을 이용해 3배씩 키우거나 줄입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble](_image/ensemble.png)\n",
    "\n",
    "\n",
    "위 그림처럼 여러 가지 모델들의 결과를 합쳐서 하나의 평균값으로 예측하는 것을 Ensemble이라고 합니다. Ensemble은 대체적으로 2 ~ 3% 정도 정답률이 증가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) L1, L2 Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L1 : \\sum_k \\sum_l \\lvert W_{k, l} \\rvert \\quad (W : \\text{weight decay})$$\n",
    "$$L2 : \\sum_k \\sum_l (W_{k, l})^2$$\n",
    "$$Elastic net(\\text{L1 + L2}) : \\sum_k \\sum_l (\\beta W_{k, l}^2 + \\lvert W_{k, l} \\rvert)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 모델에 여러 뉴런이 있습니다. 이 중 임의로 몇 개의 뉴런을 무시하여 다양한 모델들을 사용하는 결과를 주는, 즉 ensemble 효과를 주는 방법을 Dropout이라고 합니다. \n",
    "\n",
    "![dropout](_image/dropout.png)\n",
    "\n",
    "위 그림처럼 임의의 뉴런을 무시하고 진행하는 방식입니다. 위 방식을 여러번 사용하여 ensemble처럼 모든 모델들의 예측값의 평균을 내서 결과를 예측합니다.\n",
    "\n",
    "주의할 점은 Dropout했던 모델을 테스트할 때는 모든 노드를 사용한다는 것입니다. 그렇기에 output이 학습할 때보다 크게 나오게 됩니다. 이를 방지하기 위해 학습 때 사용한 노드의 비중만큼만 output에서 가져옵니다.\n",
    "\n",
    "예를 들어 70%의 뉴런만 사용하여 학습을 진행했다면 실제 테스트할 때도 ouput의 70%를 실제 예측값으로 사용하게 됩니다.\n",
    "\n",
    "이를 코드로 나타내면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5 # probability of keeping a unit active, higher = less dropout\n",
    "\n",
    "\n",
    "def train_step(X, W, b):\n",
    "    \"\"\"X contains the data\"\"\"\n",
    "    \n",
    "    # forward pass for example 3-layer neural network\n",
    "    H1 = np.maximum(0, np.dot(W[0], X) + b[0])\n",
    "    U1 = np.random.rand(*H1.shape) < p # first dropout mask\n",
    "    H1 *= U1 # drop\n",
    "    H2 = np.maximum(0, np.dot(W[1], H1) + b[1])\n",
    "    U2 = np.random.rand(*H2.shape) < p # second dropout mask\n",
    "    H2 *= U2 # drop\n",
    "    out = np.dot(W[2], H2) + b[2]\n",
    "    \n",
    "    # backward pass: compute gradients...(not shown)\n",
    "    # perform parameter update...(not shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    # ensemble forward pass\n",
    "    H1 = np.maximum(0, np.dot(W[0], X) + b[0]) * p # scale the activations\n",
    "    H2 = np.maximum(0, np.dot(W[1], H1) + b[1]) * p # scale the activations\n",
    "    out = np.dot(W[2], H2) + b[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Data Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가진 데이터들이 적을 때나 더 많이 필요할 때, 데이터들을 변형, 회전, 늘림 등의 과정을 통해 데이터를 늘리는 것을 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Tensor operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서는 배열이나 행렬과 매우 유사한 자료구조입니다. PyTorch에서는 텐서를 사용하여 모델의 입력과 출력뿐만 아니라 모델의 파라미터를 나타냅니다.\n",
    "\n",
    "GPU나 다른 연산 가속을 위한 특수한 하드웨어에서 실행할 수 있다는 점을 제외하면, 텐서는 NumPy의 ndarray와 매우 유사합니다.\n",
    "\n",
    "이제 텐서에 대한 다양한 구현을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터로부터 직접 생성하기\n",
    "data = [[1, 2], [3, 4]]\n",
    "x = torch.tensor(data)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy array로부터 생성하기\n",
    "np_array = np.array(data)\n",
    "x = torch.from_numpy(np_array)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor에서 numpy array로 변환하기\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]], dtype=torch.int32) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.0851, 0.0584],\n",
      "        [0.3296, 0.2676]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다른 텐서와 같은 모양의 텐서 초기화하기\n",
    "x_ones = torch.ones_like(x) # x_data의 속성을 유지합니다.\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x, dtype=torch.float) # x_data의 속성을 덮어씁니다.\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.4402, 0.1129, 0.1370, 0.3384],\n",
      "        [0.3500, 0.0489, 0.0349, 0.6645],\n",
      "        [0.6792, 0.5942, 0.9422, 0.8516]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 주어진 shape로 초기화하기\n",
    "shape = (3,4)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서의 속성은 텐서의 모양, 자료형 및 어느 장치에 저장되는지를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 노트북은 gpu가 없다\n",
    "\n",
    "#device = torch.device('cuda')\n",
    "#tensor = tensor.to(device)\n",
    "#print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 텐서간의 연산도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 식의 인덱싱과 슬라이싱\n",
    "tensor = torch.ones(3, 4)\n",
    "tensor[:, 1] = 0\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 합치기, 행으로 길어지도록\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 합치기, 열로 이어지도록\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 곱하기\n",
    "\n",
    "# 요소별 곱(element-wise product)을 계산합니다\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "\n",
    "# 다른 문법:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 간 행렬 곱셈\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# 다른 문법:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Autograd**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에는 torch.autograd라고 불리는 자동 미분 엔진이 내장되어 있습니다. autograd를 통해 입력 X, 파라미터 W , 그리고 cross-entropy loss를 사용하는 logistic regression model의 gradient를 구하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([[ 0.6696, -1.1055,  0.7509],\n",
      "        [ 0.2827,  0.7063,  1.1959],\n",
      "        [ 0.4497,  0.8117,  0.0219],\n",
      "        [ 0.3535,  0.1437,  1.7595],\n",
      "        [-0.3871,  0.4001, -1.1086]], requires_grad=True)\n",
      "tensor([-1.2913,  0.2003,  0.2606], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 입력 및 파라미터 초기화\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "print(y)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0770, 1.1566, 2.8803], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward\n",
    "z = torch.matmul(x,w)+b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서는 node를 크게 2가지 방법의 api를 활용해서 사용합니다.\n",
    "\n",
    "1. torch.nn\n",
    "2. torch.nn.functional\n",
    "\n",
    "torch.nn은 사전에 node를 초기화하고 해당 node에 텐서를 통과시켜 값을 받는 형태지만, torch.nn.functional은 사전에 초기화없이 바로 함수처럼 사용하는 방식입니다.\n",
    "\n",
    "코딩 스타일에 맞춰서 원하시는 api를 사용하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6991, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비용 함수\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(z, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6991, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델에서 매개변수의 가중치를 최적화하려면 파라미터에 대한 loss function의 도함수(derivative)를 계산해야 합니다. \n",
    "이러한 도함수를 계산하기 위해, loss.backward() 를 호출한 다음 w.grad와 b.grad에서 값을 가져옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156]])\n",
      "tensor([0.1731, 0.2536, 0.3156])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로, requires_grad=True인 모든 텐서들은 연산 기록을 추적하고 미분 계산을 지원합니다. 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 forward 연산만 필요한 경우에는, 미분 연산을 위한 값들을 저장해두는 것이 속력 및 메모리의 저하를 가져올 수 있습니다. 연산 코드를 torch.no_grad() 블록으로 둘러싸서 미분 추적을 멈출 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습2. LR vs MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 MNIS dataset을 활용하여 logistic regression model과 MLP model을 구현해보고 학습 파이프라인을 익혀보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available is True else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Preprocess Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mnist = fetch_openml('mnist_784', cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist에 존재하는 각각의 사진은 28*28의 픽셀로 구성된 784차원짜리 벡터로 나타나져 있습니다. 각 픽셀은 0~255 사이의 값으로 흰색부터 검은색 사이의 값을 나타냅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# preprocess dataset\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "X = X.values\n",
    "y = y.values\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# scale\n",
    "X /= 255.0\n",
    "print(X.min(), X.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(56000,)\n",
      "(7000, 784)\n",
      "(7000,)\n",
      "(7000, 784)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "print(X_train.shape) # 80%\n",
    "print(y_train.shape)\n",
    "print(X_val.shape) # 10%\n",
    "print(y_val.shape)\n",
    "print(X_test.shape) # 10%\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize dataset\n",
    "def plot_example(X, y):\n",
    "    \"\"\"Plot the first 5 images and their labels in a row.\"\"\"\n",
    "    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):\n",
    "        plt.subplot(151 + i)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(y)\n",
    "        \n",
    "\n",
    "plot_example(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) PyTorch Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서는 Custom Dataset을 사용하기 위해서는 torch.utils.data.Dataset의 형태로 dataset class를 정의해준 이후, torch.utils.data.DataLoader의 형태로 dataloader class를 정의하여 학습시에 model에 forwarding할 data를 sample해줍니다.\n",
    "\n",
    "(https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)\n",
    "\n",
    "\n",
    "가장 보편적으로 사용되는 map-style의 dataset class는 torch.utils.data.Dataset을 superclass로 받아 **getitem()** 과 **len()** 함수를 override해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.y[index]\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(np.array(y)).long()\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000\n",
      "(56000, 784)\n",
      "7000\n",
      "(7000, 784)\n",
      "7000\n",
      "(7000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.X.shape)\n",
    "print(len(val_dataset))\n",
    "print(val_dataset.X.shape)\n",
    "print(len(test_dataset))\n",
    "print(test_dataset.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader는 train 혹은 validation시 dataset에서 batch를 sampling하기 위한 API입니다 (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "필수적으로 사용하는 option들은 아래와 같습니다.\n",
    "- dataset: sampling할 dataset\n",
    "- batch_size: 한번에 sampling할 dataset의 개수\n",
    "- shuffle: 1 epoch를 기준으로 dataset을 shuffle할지\n",
    "\n",
    "더 자세한 option은 api를 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875\n",
      "110\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# shuffle the train data\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# do not shuffle the val & test data\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset size // batch_size\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch에서 model을 선언할 때는 torch.nn.Module class를 superclass로 받아 __init__()함수와 forward() 함수를 작성해줍니다.\n",
    "\n",
    "__init__()함수에는 모델의 파라미터들을 선언하고, forward함수에는 해당 파라미터들을 이용하여 data를 model에 통과시켜줍니다.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression Model\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LR, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLP Model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 선언한 model을 통해 학습을 진행하기 위해선 파라미터를 최적화할 optimizer가 필요합니다. 이번 실습에선 가장 보편적으로 사용되는 Adam optimizer를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "class Trainer():\n",
    "    def __init__(self, trainloader, valloader, testloader, model, optimizer, criterion, device):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        model: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.testloader = testloader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        # 학습을 시작하기 위해 model을 train-mode로 변경\n",
    "        self.model.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, data in enumerate(self.trainloader, 0): \n",
    "                inputs, labels = data \n",
    "                # model에 input으로 tensor를 gpu-device로 보낸다\n",
    "                inputs = inputs.to(self.device)  \n",
    "                labels = labels.to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                # optimizer는 예전 기울기도 계속 저장하기에 기울기를 초기화해준다.\n",
    "                self.optimizer.zero_grad()    \n",
    "                # forward + backward + optimize\n",
    "                # get output after passing through the network\n",
    "                outputs = self.model(inputs) \n",
    "                # compute model's score using the loss function\n",
    "                loss = self.criterion(outputs, labels)  \n",
    "                # perform back-propagation from the loss\n",
    "                loss.backward() \n",
    "                # gradient descent를 통해 model의 output을 얻는다.\n",
    "                self.optimizer.step() \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print('epoch: %d  loss: %.3f' % (e + 1, running_loss / len(self.trainloader)))\n",
    "            running_loss = 0.0\n",
    "        val_acc = self.validate()\n",
    "        return val_acc\n",
    "\n",
    "    def validate(self):\n",
    "        # 현재 model이 train-mode일 수 있기에 eval-mode로 바꿔 validate를 수행할 수 있도록 변경\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.valloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        return correct / len(self.valloader.dataset)\n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        return correct / len(self.testloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.166\n",
      "epoch: 2  loss: 1.194\n",
      "epoch: 3  loss: 1.261\n",
      "epoch: 4  loss: 1.297\n",
      "val_acc: 0.886\n",
      "epoch: 1  loss: 0.356\n",
      "epoch: 2  loss: 0.305\n",
      "epoch: 3  loss: 0.296\n",
      "epoch: 4  loss: 0.294\n",
      "val_acc: 0.919\n",
      "epoch: 1  loss: 0.555\n",
      "epoch: 2  loss: 0.325\n",
      "epoch: 3  loss: 0.295\n",
      "epoch: 4  loss: 0.281\n",
      "val_acc: 0.924\n",
      "epoch: 1  loss: 1.387\n",
      "epoch: 2  loss: 0.735\n",
      "epoch: 3  loss: 0.557\n",
      "epoch: 4  loss: 0.474\n",
      "val_acc: 0.893\n",
      "test_acc: 0.919\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "epoch = 4\n",
    "\n",
    "best_acc = 0.0\n",
    "lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for lr in lrs:\n",
    "    model = LR(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)\n",
    "    val_acc = trainer.train(epoch = epoch)\n",
    "    print('val_acc: %.3f' %(val_acc))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'models/3-1_logistic_model')\n",
    "\n",
    "trainer.model.load_state_dict(torch.load('models/3-1_logistic_model'))\n",
    "test_acc = trainer.test()\n",
    "print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.930\n",
      "epoch: 2  loss: 0.956\n",
      "epoch: 3  loss: 1.004\n",
      "epoch: 4  loss: 1.003\n",
      "val_acc: 0.678\n",
      "epoch: 1  loss: 0.278\n",
      "epoch: 2  loss: 0.170\n",
      "epoch: 3  loss: 0.148\n",
      "epoch: 4  loss: 0.133\n",
      "val_acc: 0.956\n",
      "epoch: 1  loss: 0.472\n",
      "epoch: 2  loss: 0.254\n",
      "epoch: 3  loss: 0.206\n",
      "epoch: 4  loss: 0.173\n",
      "val_acc: 0.950\n",
      "epoch: 1  loss: 1.298\n",
      "epoch: 2  loss: 0.553\n",
      "epoch: 3  loss: 0.414\n",
      "epoch: 4  loss: 0.358\n",
      "val_acc: 0.911\n",
      "test_acc: 0.953\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "input_dim = 784\n",
    "hidden_dim = 32\n",
    "output_dim = 10\n",
    "epoch = 4\n",
    "\n",
    "best_acc = 0.0\n",
    "lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for lr in lrs:\n",
    "    model = MLP(input_dim=input_dim, \n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)\n",
    "    val_acc = trainer.train(epoch = epoch)\n",
    "    print('val_acc: %.3f' %(val_acc))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'models/3-1_MLP_model')\n",
    "\n",
    "trainer.model.load_state_dict(torch.load('models/3-1_MLP_model'))\n",
    "test_acc = trainer.test()\n",
    "print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습3. Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 activation function 중 가장 대표적으로 사용되는 sigmoid functio과 ReLU function을 사용해보고 비교해보겠습니다. 데이터는 실습 2에서 사용했던 MNIST를 사용합니다. 모델은 train과 test만 사용하겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xfJBd9v9L_RgXGf8urNrYpb40zXU6gea)\n",
    "\n",
    "- input: 784\n",
    "- hidden: 32 or (32, 32)\n",
    "- output: 10\n",
    "- **activation: sigmoid or relu**\n",
    "- optimizer: sgd\n",
    "- loss: cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) Prerequisite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available is True else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oCnmrA9ltYs0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "X = mnist.data.astype('float32').values\n",
    "y = mnist.target.astype('int64').values\n",
    "X /= 255.0\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB-u3e9taDjT"
   },
   "source": [
    "#### **Split Dataset**\n",
    "\n",
    "학습과 평가를 위한 dataset으로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-HWWRcNjaLDi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(56000,)\n",
      "(14000, 784)\n",
      "(14000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYnvqbdijWUQ"
   },
   "source": [
    "#### **Pytorch Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ypqp7zA-xRlB"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.y[index]\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(np.array(y)).long()\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hTr4OWatzmaU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000\n",
      "(56000, 784)\n",
      "14000\n",
      "(14000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.X.shape)\n",
    "print(len(test_dataset))\n",
    "print(test_dataset.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51PT-uPVzE8_"
   },
   "source": [
    "#### **DataLoader**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x2k3YVBoxRnF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875\n",
      "219\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# shuffle the train data\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# do not shuffle the val & test data\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset size // batch_size\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN65oTBk1d4T"
   },
   "source": [
    "#### **Trainer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OJqIwSltn9uY"
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, trainloader, testloader, model, optimizer, criterion, device):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        model: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        self.model.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, data in enumerate(self.trainloader, 0): \n",
    "                inputs, labels = data \n",
    "                # model에 input으로 tensor를 gpu-device로 보낸다\n",
    "                inputs = inputs.to(self.device)  \n",
    "                labels = labels.to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()    \n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(inputs) \n",
    "                loss = self.criterion(outputs, labels)  \n",
    "                loss.backward() \n",
    "                self.optimizer.step() \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print('epoch: %d  loss: %.3f' % (e + 1, running_loss / len(self.trainloader)))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        test_acc = correct / len(self.testloader.dataset)\n",
    "        print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 2-layer Network + Sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=32, \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.187\n",
      "epoch: 2  loss: 1.817\n",
      "epoch: 3  loss: 1.386\n",
      "epoch: 4  loss: 1.079\n",
      "epoch: 5  loss: 0.882\n",
      "epoch: 6  loss: 0.754\n",
      "epoch: 7  loss: 0.666\n",
      "epoch: 8  loss: 0.602\n",
      "epoch: 9  loss: 0.553\n",
      "epoch: 10  loss: 0.515\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.877\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 2-layer Network + ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=32, \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.298\n",
      "epoch: 2  loss: 0.527\n",
      "epoch: 3  loss: 0.409\n",
      "epoch: 4  loss: 0.364\n",
      "epoch: 5  loss: 0.340\n",
      "epoch: 6  loss: 0.323\n",
      "epoch: 7  loss: 0.310\n",
      "epoch: 8  loss: 0.299\n",
      "epoch: 9  loss: 0.290\n",
      "epoch: 10  loss: 0.282\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.917\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function에 따른 성능 차이가 보입니다. Sigmoid는 기울기 소실 문제가 발생하지만 그에 비해 ReLU는 기울기 소실 문제가 없기 때문입니다. 이제 층을 늘려보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 3-layer Network + Sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.303\n",
      "epoch: 2  loss: 2.295\n",
      "epoch: 3  loss: 2.288\n",
      "epoch: 4  loss: 2.278\n",
      "epoch: 5  loss: 2.259\n",
      "epoch: 6  loss: 2.221\n",
      "epoch: 7  loss: 2.147\n",
      "epoch: 8  loss: 2.027\n",
      "epoch: 9  loss: 1.887\n",
      "epoch: 10  loss: 1.741\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.504\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 3-layer Network + ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.875\n",
      "epoch: 2  loss: 0.635\n",
      "epoch: 3  loss: 0.415\n",
      "epoch: 4  loss: 0.356\n",
      "epoch: 5  loss: 0.326\n",
      "epoch: 6  loss: 0.305\n",
      "epoch: 7  loss: 0.288\n",
      "epoch: 8  loss: 0.273\n",
      "epoch: 9  loss: 0.261\n",
      "epoch: 10  loss: 0.248\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.928\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) 결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 4가지 경우의 accuracy를 표로 정리하겠습니다.\n",
    "\n",
    "||Sigmoid|ReLU|\n",
    "|:---:|:---:|:---:|\n",
    "|2-layer|0.877|0.917|\n",
    "|3-layer|0.504|0.928|\n",
    "\n",
    "ReLU의 경우 어느 정도 비슷해보이지만 Sigmoid의 경우 accuracy가 거의 절반으로 떨어진 것을 볼 수 있습니다. \n",
    "\n",
    "Sigmoid의 경우 층이 추가되면서 기울기 소실이 일어난 것으로 보입니다. 그에 반해 ReLU는 기울기 소실이 발생하지 않기에 층이 쌓여도 정확도가 떨어지지는 않습니다. \n",
    "\n",
    "만약 activation function이 없다면 가중치와 입력값의 곱으로만 뉴런이 이루어집니다. 이는 non-linear한 데이터를 표현할 수 없음을 의미합니다. 그리고 층을 여러개 쌓일 필요도 없어집니다. 그렇기에 non-linear한 데이터도 잘 표현하기 위해서 activation function은 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습4. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습 3에 이어서 이번 실습에선 sgd, momentun, Adam 등의 optimizer를 사용해보고 성능을 비교해보겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xfCTx8xj4zoaombrK2bSN9nv0Z3r95jp)\n",
    "\n",
    "- input: 784\n",
    "- hidden: (32, 32)\n",
    "- output: 10\n",
    "- activation: relu\n",
    "- **optimizer: sgd** or **momentum** or **adam**\n",
    "- loss: cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) MLP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 3-layer Network + ReLU + SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.749\n",
      "epoch: 2  loss: 0.618\n",
      "epoch: 3  loss: 0.428\n",
      "epoch: 4  loss: 0.370\n",
      "epoch: 5  loss: 0.338\n",
      "epoch: 6  loss: 0.316\n",
      "epoch: 7  loss: 0.298\n",
      "epoch: 8  loss: 0.282\n",
      "epoch: 9  loss: 0.268\n",
      "epoch: 10  loss: 0.256\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.923\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 3-layer Network + ReLU + Momentum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.99)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.600\n",
      "epoch: 2  loss: 0.255\n",
      "epoch: 3  loss: 0.210\n",
      "epoch: 4  loss: 0.199\n",
      "epoch: 5  loss: 0.165\n",
      "epoch: 6  loss: 0.150\n",
      "epoch: 7  loss: 0.151\n",
      "epoch: 8  loss: 0.149\n",
      "epoch: 9  loss: 0.140\n",
      "epoch: 10  loss: 0.136\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.947\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 3-layer Network + ReLU + Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.295\n",
      "epoch: 2  loss: 0.174\n",
      "epoch: 3  loss: 0.156\n",
      "epoch: 4  loss: 0.141\n",
      "epoch: 5  loss: 0.132\n",
      "epoch: 6  loss: 0.128\n",
      "epoch: 7  loss: 0.122\n",
      "epoch: 8  loss: 0.118\n",
      "epoch: 9  loss: 0.116\n",
      "epoch: 10  loss: 0.113\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.958\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer에 따른 정확도를 표로 나타내면 다음과 같다.\n",
    "\n",
    "|SGD|Momentum|Adam|\n",
    "|:---:|:---:|:---:|\n",
    "|0.923|0.947|0.958|\n",
    "\n",
    "Adam > Momentum > SGD 순임을 알 수 있습니다. 평균적으로 학습 속도도 정확도가 높을수록 짧습니다. SGD는 현재 위치의 기울기만을 가지고 경사 하강을 하기에 진동이 심합니다. 이에 반해 Momentum은 관성의 개념을 이용하여 진동을 줄이고 더욱 빠르게 저점으로 수렴하게 만듭니다. 그렇기에 SGD보다 Momentum이 더 빠르게 학습합니다. Adam은 Momentum의 방법에 이전 기울기까지 고려하여 이동합니다. 그렇기에 Momentum보다 더 빠르게 수렴하고 학습속도도 더 빠릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습5. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 image data에서 주로 사용되는 batch-normalization까지 추가해보겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xZSWZiSxuGZAsonghidhTSfUEYiuxRtN)\n",
    "\n",
    "- input: 784\n",
    "- hidden: 32 or (32, 32)\n",
    "- output: 10\n",
    "- activation: relu\n",
    "- optimizer: adam\n",
    "- **regularizer: batch_norm**\n",
    "- loss: cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-layer Network + ReLU + Adam + batch_norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.260\n",
      "epoch: 2  loss: 0.146\n",
      "epoch: 3  loss: 0.121\n",
      "epoch: 4  loss: 0.101\n",
      "epoch: 5  loss: 0.092\n",
      "epoch: 6  loss: 0.085\n",
      "epoch: 7  loss: 0.080\n",
      "epoch: 8  loss: 0.073\n",
      "epoch: 9  loss: 0.070\n",
      "epoch: 10  loss: 0.068\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.972\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26634"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch-normalization을 쓴 것과 쓰지 않은 것을 비교해보면 다음과 같습니다.\n",
    "\n",
    "|Batch-normalization O|Batch-normalization X|\n",
    "|:---:|:---:|\n",
    "|0.972|0.958|\n",
    "\n",
    "Batch-normalization을 썼을 때, 정확도가 더 증가한 것을 확인할 수 있습니다. Regularization은 feature들의 영향력을 조절하여 overfitting을 방지하는 역할을 합니다. 우리의 모델은 feature가 많음을 알 수 있습니다. 그렇기에 batch-normalization으로 overfitting을 방지하여 더 일반적이고 성능이 좋은 학습이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Convolution Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/62306 의 자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 합성곱과 풀링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**합성곱 신경망(Convolutional Neural Network)** 은 이미지 처리에 탁월한 성능을 보이는 신경망입니다. 합성곱 신경망은 크게 **합성곱층(Convolution layer)** 과 **풀링층(Pooling layer)** 으로 구성됩니다. 아래의 그림은 합성곱 신경망의 일반적인 예를 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/62306/convpooling.PNG\">\n",
    "\n",
    "위 그림에서 CONV는 합성곱 연산을 의미하고, 합성곱 연산의 결과가 활성화 함수 ReLU를 지납니다. 이 두 과정을 합성곱층이라고 합니다. 그 후에 POOL이라는 구간을 지나는데 이는 풀링 연산을 의미하여 풀링층이라고 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 합성곱 신경망의 대두"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 처리를 하기 위해서 앞서 배운 다층 퍼셉트론을 사용할 수는 있지만 한계가 있었습니다. 예를 들어, 알파벳 손글씨를 분류하는 어떤 문제가 있습니다. 알파벳 y를 손글씨로 쓴 두 가지 예시를 행렬로 표현한 것이 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv0.png\">\n",
    "\n",
    "사람이 보기에 두 그림은 모두 y로 보이지만 기계가 보기에 각 픽셀마다 가진 값이 거의 상이하므로 완전히 다른 입력으로 받아들입니다. 그리고 두 이미지 외에도 휘어지거나, 이동되거나 방향이 뒤틀리는 등 다양한 변형이 존재합니다. 다층 퍼셉트론은 몇 가지 픽셀만 값이 달라져도 민감하게 받아들이기에 적합하지 않습니다. \n",
    "\n",
    "이를 더 자세히 살펴봅시다. 만약 위 이미지를 다층 퍼셉트론으로 분류한다면 이미지를 1차원 텐서인 벡터로 변환하여 입력층으로 사용해야 합니다. 이를 전환하면 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv1.png\">\n",
    "\n",
    "1차원으로 변환된 결과는 사람이 보기에 원래 이미지를 유추하기 매우 어렵습니다. 이는 기계 역시 마찬가집니다. 위와 같은 변환은 전에 가지고 있던 공간적인 구조(spatial structure) 정보가 유실된 상태입니다. 그렇기에 다층 퍼셉트론으로 이미지를 분류하기엔 어렵습니다. 그렇기에 이미지의 공간적인 구조 정보를 보존하면서 학습할 수 있는 방법이 필요해졌고 이를 위해 사용하는 것이 합성곱 신경망입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 채널(Channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 처리의 기본적인 용어인 채널에 대해서 간단히 정의하겠습니다.\n",
    "\n",
    "기계는 글자나 이미지보다, 텐서를 더 잘 처리할 수 있습니다. 이미지는 **(높이, 너비, 채널)** 이라는 3차원 텐서입니다. 여기서 높이는 세로 방향 픽셀 수, 너비는 이미지의 가로 방향 픽셀 수, 채널은 색 성분을 의미합니다. 흑백 이미지는 채널 수가 1이며 각 픽셀은 0부터 255 사이의 값을 가집니다. 아래는 28 x 28 픽셀의 손글씨 데이터를 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv2.png\">\n",
    "\n",
    "위 손글씨 데이터는 흑백 이미지이므로 채널 수가 1입니다. 그렇기에 위 이미지는 (28 x 28 x 1)의 크기를 가지는 3차원 텐서입니다. 만약 컬러 이미지라면 RGB가 각각 채널이 1개씩 총 3개를 가집니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv3.png\">\n",
    "\n",
    "하나의 픽셀은 삼원색의 조합으로 이루어집니다. 만약, 높이와 너비가 28인 컬러 이미지가 있다면 이 이미지의 텐서는 (28 x 28 x 3)의 크기를 가지는 3차원 텐서입니다. 채널은 떄로는 깊이(depth)라고도 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 합성곱 연산(Convolution operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱층은 합성곱 연산을 통해서 이미지의 특징을 추출하는 역할을 합니다. 합성곱은 kernel 또는 filter라는 n x m크기의 행렬로 height x width 크기의 이미지를 처음부터 끝까지 겹치고 훓으면서 n x m 크기의 겹쳐지는 부분의 각 이미지와 kernel의 원소의 값을 곱합니다. 그리고 이를 모두 더하여 값으로 출력하는 것을 말합니다. \n",
    "\n",
    "- kernel은 일반적으로 3 x 3, 5 x 5를 많이 사용합니다.\n",
    "\n",
    "예시를 보겠습니다. 아래는 3 x 3크기의 커널로 5 x 5 이미지 행렬에 합성곱 연산을 수행하는 과정을 보여줍니다. 한 번의 연산을 1 step이라고 했을 때, 합성곱 연산의 네번째 스텝까지 이미지와 식으로 보겠습니다. \n",
    "\n",
    "1) 첫번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv4.png\">\n",
    "\n",
    "$(1 \\times 1) + (2 \\times 0) + (3 \\times 1) + (2 \\times 1) + (1 \\times 0)+ (0 \\times 1)+ (3 \\times 0)+ (0 \\times 1) + (1 \\times 0) = 6$\n",
    "\n",
    "\n",
    "2) 두번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv5.png\">\n",
    "\n",
    "$(2 \\times 1) + (3 \\times 0) + (4 \\times 1) + (1 \\times 1) + (0 \\times 0)+ (1 \\times 1)+ (0 \\times 0)+ (1 \\times 1) + (1 \\times 0) = 9$\n",
    "\n",
    "\n",
    "3) 세번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv6.png\">\n",
    "\n",
    "$(3 \\times 1) + (4 \\times 0) + (5 \\times 1) + (0 \\times 1) + (1 \\times 0)+ (2 \\times 1)+ (1 \\times 0)+ (1 \\times 1) + (0 \\times 0) = 11$\n",
    "\n",
    "\n",
    "4) 네번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv7.png\">\n",
    "\n",
    "$(2 \\times 1) + (1 \\times 0) + (0 \\times 1) + (3 \\times 1) + (0 \\times 0)+ (1 \\times 1)+ (1 \\times 0)+ (4 \\times 1) + (1 \\times 0) = 10$\n",
    "\n",
    "\n",
    "총 9번의 스텝을 했을 때 최종 결과는 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv8.png\">\n",
    "\n",
    "위와 같이 입력으로부터 커널을 사용하여 합성곱 연산을 통해 나온 결과를 **특성 맵(feature map)** 이라고 합니다.\n",
    "\n",
    "위의 예제에선 커널의 크기 3 x 3이었지만, 커널의 크기는 사용자가 지정할 수 있습니다. 또한 커널의 이동 범위가 위의 예제에서는 한 칸이었지만, 이 또한 사용자가 정할 수 있습니다. 이러한 이동 범위를 **스트라이드(stride)** 라고 합니다.\n",
    "\n",
    "아래의 예제는 스트라이드가 2일 때, 5 x 5 이미지에 합성곱 연산을 수행하는 3 x 3 커널의 움직임을 보여줍니다. 최종적으로 2 x 2 크기의 특성 맵을 얻습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 패딩(Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예시처럼 합성곱 연산으로 얻은 특성 맵은 입력보다 크기가 작아지는 특징이 있습니다. 만약, 합성곱 층을 여러개 쌓았다면 최종적으로 얻는 특성 맵의 크기는 입력보다 매우 작아진 상태가 됩니다. 만약 합성곱 연산 이후에도 특성 맵의 크기가 입력 크기와 동일하게 유지되도록 하고 싶다면 패딩(padding)을 사용합니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv10.png\">\n",
    "\n",
    "패딩은 합성곱 연산을 하기 전에 입력의 가장자리에 지정된 개수의 폭만큼 행과 열을 추가하는 것을 말합니다. 다시 말해 지정된 개수의 폭만큼 테두리를 추가합니다. 주로 0으로 채우는 제로 패딩(zeor padding)을 사용합니다. 위의 그림은 5 x 5 이미지에 1폭짜리 제로 패딩을 사용한 모습입니다.\n",
    "\n",
    "만약 스트라이드가 1일 때, 3 x 3 크기의 커널을 사용한다면 1폭짜리 제로 패딩을 사용하고, 5 x 5 크기의 커널을 사용한다면 2폭짜리 제로 패딩을 사용하여 크기를 보존할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 가중치와 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 신경망에서의 가중치와 편향을 이해하기 위해 먼저 다층 퍼셉트론을 복습하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 합성곱 신경망의 가중치**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 퍼셉트론으로 3 x 3 이미지를 처리한다고 가정하겠습니다. 우선 이미지를 1차원 텐서로 만들면 입력층은 9개의 뉴론을 가집니다. 그리고 4개의 뉴론을 가지는 은닉층을 추가한다면 아래의 그림과 같아집니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv11.png\">\n",
    "\n",
    "위에서 각 연결선은 가중치를 의미하므로 위의 그림에서는 36(=9 x 4)개의 가중치를 가집니다.\n",
    "\n",
    "같은 이미지를 합성곱 신경망으로 처리해보겠습니다. 2 x 2 커널을 사용하고 스트라이드는 1로 하겠습니다. (*는 합성곱 연산을 의미합니다.)\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv12.png\">\n",
    "\n",
    "합성곱 신경망에서 가중치는 커널 행렬의 원소들입니다. 이를 인공 신경망으로 표현하면 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv13.png\">\n",
    "\n",
    "최종적으로 특성 맵을 얻기 위해서 동일한 커널로 이미지 전체를 훑으면서 합성곱 연산을 진행합니다. 결국 이미지 전체를 훑으면서 사용되는 가중치는 $w_0, w_1, w_2, w_3$ 4개 뿐입니다. 그리고 각 합성곱 연산마다 이미지의 모든 픽셀을 사용하는 것이 아니라 커널과 맵핑되는 픽셀만을 입력으로 사용하는 것을 볼 수 있습니다. 결국 합성곱 신경망은 다층 퍼셉트론보다 훨씬 적은 가중치를 사용하며 공간적 구조 정보를 보존하는 특징을 가집니다.\n",
    "\n",
    "다층 퍼셉트론의 은닉층에서 가중치 연산 이후, 비선형성을 위해 활성화 함수를 통과시켰듯, 합성곱 신경망에서도 합성곱 연산을 통해 얻은 특성 맵을 활성화 함수를 통과시켜 비선형성을 갖게 만듭니다. 이때 활성화 함수로 ReLU나 그 변형들이 주로 사용됩니다. 이와 같이 합성곱 연산을 통해 특성 맵을 얻고, 활성화 함수를 지나는 연산을 하는 합성곱 신경망의 은닉층을 **합성곱 층(convolution layer)** 이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 합성곱 신경망의 편향**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/64066/conv14.png\">\n",
    "\n",
    "합성곱 신경망에도 편향을 추가할 수 있습니다. 만약, 편향을 사용한다면 커널을 적용한 뒤에 더해집니다. 편향은 하나의 값만 존재하며 커널이 적용된 결과의 모든 원소에 더해집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 특성 맵의 크기 계산 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 연산의 notation을 먼저 확인하겠습니다.\n",
    "\n",
    "- $I_h$: 입력의 높이\n",
    "- $I_w$: 입력의 너비\n",
    "- $K_h$: 커널의 높이\n",
    "- $K_w$: 커널의 너비\n",
    "- $S$: 스트라이드\n",
    "- $O_h$: 특성 맵의 높이\n",
    "- $O_w$: 특성 맵의 너비\n",
    "\n",
    "이에 따라 특성 맵의 높이와 너비는 다음과 같습니다.\n",
    "\n",
    "$$O_h = floor(\\frac{I_h - K_h}{S} + 1)$$\n",
    "\n",
    "$$O_w = floor(\\frac{I_w - K_w}{S} + 1)$$\n",
    "\n",
    "여기서 $floor$ 함수는 소수점 발생 시, 소수점 이하를 버리는 역할을 합니다. \n",
    "\n",
    "예를 들어 5 x 5 크기의 이미지에 3 x 3 커널을 사용하고 스트라이드 1로 합성곱 연산을 한 경우, 특성 맵의 크기는 (5 - 3 + 1) x (5 - 3 + 1) = 3 x 3임을 알 수 있습니다. 이는 또한 9번의 스텝이 필요함을 의미하기도 합니다.\n",
    "\n",
    "패딩의 폭을 $P$라고 할=고, 패딩까지 고려한 식은 다음과 같습니다.\n",
    "\n",
    "$$O_h = floor(\\frac{I_h - K_h + 2P}{S} + 1)$$\n",
    "\n",
    "$$O_w = floor(\\frac{I_w - K_w + 2P}{S} + 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 다수의 채널을 가질 경우의 합성곱 연산(3차원 텐서의 합성곱 연산)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 채널 또는 깊이를 고려하지 않고, 2차원 텐서를 가정하고 설명했습니다. 하지만 실제로 합성곱 연산의 입력은 '다수의 채널을 가진' 이미지 또는 이전 연산의 결과로 나온 특성 맵일 수 있습니다. 만약, 다수의 채널을 가진 입력 데이터를 가지고 합성곱 연산을 한다고 하면 커널의 채널 수도 입력의 채널 수만큼 존재해야 합니다. 다시 말해 입력 데이터의 채널 수와 커널의 채널 수는 같아야 합니다. 채널 수가 같으므로 합성곱 연산을 채널마다 수행합니다. 그리고 그 결과를 모두 더하여 최종 특성 맵을 얻습니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv15.png\">\n",
    "\n",
    "위 그림은 3개의 채널을 가진 입력 데이터와 3개의 채널을 가진 커널의 합성곱 연산을 보여줍니다. 커널의 각 채널끼리의 크기는 같아야 합니다. 각 채널간 합성곱 연산을 마치고, 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵을 만듭니다. 주의할 점은 위의 연산에서 사용되는 커널은 3개의 커널이 아니라 3개의 채널을 가진 1개의 커널이라는 것입니다.\n",
    "\n",
    "위 그림은 높이 3, 너비 3, 채널 3의 입력이 높이 2, 너비 2, 채널 3의 커널과 합성곱 연산을 하여 높이 2, 너비 2, 채널 1의 특성 맵을 얻는다는 의미입니다. 합성곱 연산의 결과로 얻은 특성 맵의 태널 차원은 RGB 채널 등과 같은 컬러의 의미를 담고 있지 않습니다.\n",
    "\n",
    "이제 이 연산에서 각 차원을 변수로 두고 좀 더 일반화시켜보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 3차원 텐서의 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반화를 위해 사용하는 변수들은 다음과 같습니다.\n",
    "\n",
    "- $I_h$: 입력의 높이\n",
    "- $I_w$: 입력의 너비\n",
    "- $K_h$: 커널의 높이\n",
    "- $K_w$: 커널의 너비\n",
    "- $O_h$: 특성 맵의 높이\n",
    "- $O_w$: 특성 맵의 너비\n",
    "- $C_i$: 입력 데이터의 채널\n",
    "\n",
    "다음은 3차원 텐서의 합성곱 연산을 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv16_final.png\">\n",
    "\n",
    "높이 $I_h$, 너비 $I_w$, 채널 $C_i$의 입력 데이터는 동일한 채널 수 $C_i$를 가지는 높이 $K_h$, 너비 $K_w$의 커널과 합성곱 연산을 하여 높이 $O_h$, 너비 $O_w$, 채널 1의 특성 맵을 얻습니다. 그런데 하나의 입력에 여러 개의 커널을 사용하는 합성곱 연산을 할 수도 있습니다. \n",
    "\n",
    "합성곱 연산에서 다수의 커널을 사용할 경우, 특성 맵의 크기가 어떻게 바뀌는지 봅시다. 다음은 $C_o$를 합성곱 연산에 사용하는 커널의 수라고 했을 때의 합성곱 연산 과정을 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv17_final_final.PNG\">\n",
    "\n",
    "합성곱 연산에서 다수의 커널을 사용할 경우, 사용한 커널 수는 합성곱 연산의 결과로 나오는 특성 맵의 채널 수가 됩니다.\n",
    "\n",
    "이를 이해했다면 커널의 크기와 입력 데이터의 채널 수 $C_i$와 특성 맵(출력 데이터)의 채널 수 $C_o$가 주어졌을 때, 가중치 매개변수의 총 개수를 구할 수 있습니다. 가중치는 커널의 원소들이므로 하나의 커널의 하나의 채널은 $K_i \\times K_o$개의 매개변수를 가지고 있습니다. 그런데 합성곱 연산을 하려면 커널은 입력 데이터의 채널 수와 동일한 채널 수를 가져야 합니다. 이에 따라 하나의 커널이 가지는 매개변수의 수는 $K_i \\times K_o \\times C_i$입니다. 그런데 이러한 커널이 총 $C_o$개가 있어야 하므로 가중치 매개변수의 총 수는 다음과 같습니다.\n",
    "\n",
    "가중치 매개변수의 총 수: $K_i \\times K_o \\times C_i \\times C_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 풀링(Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 합성곱 층(합성곱 연산 + 활성화 함수) 다음에는 풀링 층을 추가하는 것이 일반적입니다. 풀링 층에서는 특성 맵을 다운샘플링하여 특성 맵의 크기를 줄이는 풀링 연산이 이루어집니다. 풀링 연산에는 일반적으로 최대 풀링(max pooling)과 평균 풀링(average pooling)이 사용됩니다. 우선 최대 풀링을 통해 풀링 연산을 보겠습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/62306/maxpooling.PNG\">\n",
    "\n",
    "풀링 연산에서도 합성곱 연산과 마찬가지로 커널과 스트라이드의 개념을 가집니다. 위의 그림은 스트라이드가 2일 때, 2 x 2 크기 커널로 맥스 풀링 연산과정입니다. 특성맵이 절반의 크기로 다운샘플링되는 것을 볼 수 있습니다. 맥스풀링은 커널과 겹치는 영역 안에서 최대값을 추출하는 방식으로 다운샘플링합니다.\n",
    "\n",
    "평균 풀링은 최대값대신 평균값을 추출하는 연산이 됩니다. 풀링 연산은 커널가 스트라이드 개념이 존재한다는 점에서 합성곱 연산과 유사하지만 합성곱 연산과 차이점은 학습해야 할 가중치가 없으며 연산 후에 채널 수가 변하지 않는다는 점입니다. \n",
    "\n",
    "풀링을 사용하면, 특성맵의 크기가 줄어드므로 특성맵의 가중치 개수를 줄여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. Fully-Connected Layer vs Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챕터1의 실습들을 통해 model의 다양한 node를 바꿔가며 mnist의 성능 변화를 확인해보았습니다. 비록, fully-connected network가 mnist 데이터에서 높은 성능을 내는데 문제가 없었지만, 모든 layer를 fully-connected layer로 만드는 것은 엄청난 파라미터와 연산량을 필요로 합니다. 그렇기에 큰 고화질의 이미지 데이터를 처리하는데 적합합지 않습니다.\n",
    "\n",
    "따라서, 이번 실습은 이미지 데이터 처리에 주로 사용되는 convolution layer를 사용하여 파라미터 수와 성능이 어떻게 변화하는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convolution Operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1xdjTf4ab0P8qfu_TaLJ4TZzt5sk3twS6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                output_dim=10):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                            out_channels=8,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8,\n",
    "                            out_channels=8,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.fc = nn.Linear(3*3*8, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # should reshape data into image\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(-1, 3*3*8)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv2): Conv2d(8, 8, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (fc): Linear(in_features=72, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Conv()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.204\n",
      "epoch: 2  loss: 0.098\n",
      "epoch: 3  loss: 0.089\n",
      "epoch: 4  loss: 0.086\n",
      "epoch: 5  loss: 0.078\n",
      "epoch: 6  loss: 0.081\n",
      "epoch: 7  loss: 0.079\n",
      "epoch: 8  loss: 0.076\n",
      "epoch: 9  loss: 0.079\n",
      "epoch: 10  loss: 0.074\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.973\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4274"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|구분|Fully-connected layer|Covolution operation|\n",
    "|:---:|:---:|:---:|\n",
    "|정확도|0.972|0.973|\n",
    "|parameter 개수|26634|4274|\n",
    "\n",
    "챕터1에서 0.972가 나온 것을 보면 0.001의 차이는 크진 않습니다. 그러나 parameter의 개수가 6배 정도 차이납니다. 이는 연산량을 줄일 수 있고 큰 데이터를 다루기에 적합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습2. 나만의 model 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챕터1, 2의 실습을 바탕으로 20,000개 이하의 파라미터를 가지며 98%이상의 정확도를 갖는 모델을 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                output_dim=10):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                            out_channels=4,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4,\n",
    "                            out_channels=8,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.fc = nn.Linear(3*3*8, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # should reshape data into image\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(-1, 3*3*8)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel()\n",
    "if count_parameters(model) > 20000:\n",
    "    raise AssertionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv2): Conv2d(4, 8, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (fc): Linear(in_features=72, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.240\n",
      "epoch: 2  loss: 0.112\n",
      "epoch: 3  loss: 0.099\n",
      "epoch: 4  loss: 0.093\n",
      "epoch: 5  loss: 0.094\n",
      "epoch: 6  loss: 0.093\n",
      "epoch: 7  loss: 0.090\n",
      "epoch: 8  loss: 0.085\n",
      "epoch: 9  loss: 0.084\n",
      "epoch: 10  loss: 0.086\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.975\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습3. Convolution Operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tqdm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Convolution Operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서는 [nn.sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) 등의 함수를 이용해서 복잡한 모델 구조를 종종 축약해서 사용하곤 합니다.\n",
    "\n",
    "아래 예제는 conv-relu-maxpool의 model을 서로 다른 방법으로 표현한 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_image = torch.rand(64, 3, 32, 32)\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **nn.Sequential**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1(nn.Module):\n",
    "    def __init__(self): # input image = batch_size x 3 x 32 x 32\n",
    "        super(Conv1, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2(nn.Module):\n",
    "    def __init__(self): # input image = batch_size x 3 x 32 x 32\n",
    "        super(Conv2, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv3(nn.Module):\n",
    "    def __init__(self): # input image = batch_size x 3 x 32 x 32\n",
    "        super(Conv3, self).__init__()\n",
    "        layer = []\n",
    "        layer.append(nn.Conv2d(3, 64, kernel_size=3, padding=1))\n",
    "        layer.append(nn.ReLU())\n",
    "        layer.append(nn.MaxPool2d(2))\n",
    "        self.layer = nn.Sequential(*layer)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model1 = Conv1()\n",
    "model2 = Conv2()\n",
    "model3 = Conv3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "torch.Size([64, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(model1)\n",
    "output = model1(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2(\n",
      "  (layer): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(model2)\n",
    "output = model2(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv3(\n",
      "  (layer): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(model3)\n",
    "output = model3(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Let's practice to calculate the shape of the network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self): # input image = batch_size x 3 x 32 x 32\n",
    "        super(Conv, self).__init__()\n",
    "        \n",
    "        # input_size, output_size, kerner, stride, padding\n",
    "        self.conv1 = nn.Conv2d(3, 512, 3, 1, 1)   # -> batch_size x 512 x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(512, 256, 3, 1, 1) # -> batch_size x 256 x 32 x 32\n",
    "        self.conv3 = nn.Conv2d(256, 256, 3, 2, 1) # -> batch_size x 256 x 16 x 16\n",
    "        self.conv4 = nn.Conv2d(256, 256, 3, 4, 1) # -> batch_size x 256 x 4 x 4\n",
    "        self.linear = nn.Linear(256*4*4, 10)      # -> batch_size x 10\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = out.contiguous().view(-1, 256*4*4)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv1): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1))\n",
      "  (linear): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Conv()\n",
    "print(model)\n",
    "output = model(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. CNN - Case Study**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 배운 CNN은 다양한 모델들이 있습니다. 그리고 이 모델을 바로 사용할 수 있습니다. 예를 들어 고양이와 강아지를 분류하는 모델이 있다면 FC층 전까지 학습된 layer를 가져오고 FC층만 이어붙여서 다른 분류 모델로 만들어 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 LeNet-5 [LeCun et al., 1998]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lenet](_image/lenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conv: 5 x 5 filter, stride = 1\n",
    "- Subsampling(Pooling) layers: 2 x 2, stride = 2\n",
    "- [CONV - POOL - CONV - POOL - FC - FC]  \n",
    "    * FC층에 돌입하기 전 1줄로 reshape를 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 AlexNet [Krizhevsky et al., 2012]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alexnet](_image/alexnet.png)\n",
    "\n",
    "위 모형에서 위쪽 그림이 잘려 있는데 이는 아래와 동일한 동작을 하기에 원저자가 위를 잘랐습니다. \n",
    "\n",
    "위에서 CONV1은 96개의 filter를 가지며 크기는 11 x 11 x 3입니다. 이렇게 다른 layer들의 정보도 확인할 수 있습니다. \n",
    "\n",
    "또한 오른쪽 learning rate는 초기값은 1e-2로 설정하고 val accuracy가 더이상 증가하지 않으면 10배씩 줄여서 진행한다는 의미입니다. 그리고 7 CNN ensemble의 %는 오차율로 줄어든 것을 확인할 수 있습니다. 다른 detail들은 읽어보면 알 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 VGGNet [Simonyan and Zisserman, 2014]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vggnet](_image/vggnet.png)\n",
    "\n",
    "VGGNet은 Alexnet보다 작은 필터, 깊은 network를 사용합니다. 위 그림을 보면 layer가 8개에서 16~19개로 늘어난 것을 확인할 수 있습니다. layer를 16개 갖는 것을 VGG16, 19개를 가지면 VGG19라고 합니다.\n",
    "\n",
    "그렇다면 왜 작은 필터를 사용했을까요??\n",
    "\n",
    "예를 들어 7 x 7 fliter로 100 x 100 x 64의 input을 처리한다고 해봅시다. 그렇다면 이때 필요한 parameter의 수는 \n",
    "\n",
    "$$(filter안의\\;parameter의 수) \\times (input의\\;depth) \\times (filter의\\;개수)$$\n",
    "\n",
    "입니다. 우리는 output과 input의 크기를 동일하게 만들 것이기에 fliter는 depth와 같이 54개입니다. 그렇기에 parameter는 총 $7 \\times 7 \\times 64 \\times 64$개가 필요합니다.\n",
    "\n",
    "이제 이 과정을 3 x 3 filter로 해보겠습니다. 3 x 3 filter로 7 x 7과 같은 동작을 해야하므로 layer가 더 필요합니다. 그렇기에 parameter의 수는\n",
    "\n",
    "$$(filter안의\\;parameter의\\;수) \\times (input의\\;depth) \\times (filter의\\;개수) \\times (7 \\times 7과\\;동일한\\;역할을\\;위해\\;필요한\\;layer의\\;수)$$\n",
    "\n",
    "입니다. 이를 대입해보면 $3 \\times 3 \\times 64 \\times 64 \\times 3$으로 7 x 7의 filter를 이용하는 것보다 parameter의 개수가 줄어든 것을 확인할 수 있습니다. 또한 layer는 증가해서 non-linear 층이 더 많아졌기에 non-linear한 표현을 더 잘할 수 있게 됩니다. \n",
    "\n",
    "![vggnet2](_image/vggnet2.png)\n",
    "\n",
    "전체 동작을 보면 위와 같습니다. 이때 CONV3은 filter의 크기가 3 x 3임을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 GoogLeNet [Szegedy et al., 2014]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet은 layer의 개수를 더 늘려서 22개를 사용한 model입니다. 특별히 Inception module을 사용합니다. 먼저 inception module을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inception Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inception](_image/inception1.png)\n",
    "\n",
    "위 사진처럼 input을 여러 개의 conv를 거치고 그것을 다시 합치는 방법입니다. 이는 feature들을 효율적으로 추출할 수 있도록 만들어줍니다. 여기서 pooling은 padding도 함께 해서 크기를 유지시켜줍니다. 그러나 이는 연산량이 굉장히 늘어납니다. 이를 위해 1 x 1 Convolution을 도입합니다.\n",
    "\n",
    "![1x1conv](_image/1x1conv.png)\n",
    "\n",
    "위에서 볼 수 있듯이 56 x 56 x 64 크기의 input을 1 x 1 conv의 filter 개수로 depth를 바꿔주는 것을 볼 수 있습니다. 즉, dimension reduction을 할 수 있게 됩니다. 그렇기에 이를 바탕으로 차원을 줄여 연산량을 줄이고 다시 차원을 늘려 과정을 진행할 수 있습니다. 그렇기에 inception module을 수정하여 밑의 그림처럼 사용합니다.\n",
    "\n",
    "![inception2](_image/inception2.png)\n",
    "\n",
    "Feature들을 효율적으로 추출하면서 연산량을 줄일 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 inception module을 이용한 GoogLeNet의 structure를 살펴보겠습니다. \n",
    "\n",
    "![googlenet](_image/googlenet.png)\n",
    "\n",
    "층이 깊게 쌓여있는 것을 볼 수 있습니다. 그리고 inception module이 쭉 이어져 있는 것을 볼 수 있습니다. \n",
    "\n",
    "여기서 눈에 띄는 두 부분이 있습니다. 가지를 치고 나온 두 부분은 softmax, 즉 output을 출력하는 곳입니다. 이는 실제 결과값에 반영하는 곳은 아닙니다. 이는 신경망이 깊어질 때 발생하는 기울기 소실 문제를 해결하기 위해 중간 층에서도 backprop을 수행하면서 가중치 갱신을 시도한다. 이는 왼쪽으로 계속 backprop되도 가중치가 더 쉽게 갱신됩니다. \n",
    "\n",
    "이는 학습 시에만 사용하고 추론 시에는 이 부분은 사용하지 않습니다.\n",
    "\n",
    "정리하자면 22개의 layer를 사용하며 그 layer는 inception module을 활용합니다. parameter의 개수가 극적으로 줄어들지만 inception module 자체가 어렵다는 특징도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5 ResNet [He et al., 2015]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet을 알아보기 전에 먼저 망을 깊게 팔 경우 어떤 문제가 생기는지 확인하겠습니다. \n",
    "\n",
    "![deeprob](_image/deepprob.png)\n",
    "\n",
    "위 그래프는 각각 20개, 56개의 layer를 가졌을 때 성능을 보여줍니다. 보면 알 수 있듯이 56개의 layer를 가진 모델이 20개의 layer를 가진 모델보다 성능이 안 좋은 것을 확인할 수 있습니다. 이는 overfitting 때문에 생긴 성능 저하는 아닙니다. \n",
    "\n",
    "이 문제에 대해 전문가들은 Optimizer가 깊은 신경망을 최적화하기엔 너무나 어렵기에 모델 성능이 안 좋아진다는 가설을 세웁니다. 그리고 적어도 얕은 모델의 성능만큼 깊은 모델도 성능이 나와야 한다는 조건으로 더 얕은 모델에서 학습된 레이어를 복사하여 학습된 모델과 함께 사용하는 방법을 구현하게 되었습니다. \n",
    "\n",
    "학습을 직접시도하는 대신 이전 복사된 학습을 이용하여 현재 학습 결과를 구하는 것입니다. 밑의 그림이 이 과정을 보여줍니다. 연결할 때는 덧셈을 사용합니다.\n",
    "\n",
    "![resnet](_image/resnet.png)\n",
    "\n",
    "이 과정을 Skip Connection이라고 합니다. 이제 ResNet의 전체 과정을 보면 다음과 같습니다.\n",
    "\n",
    "![resnet2](_image/resnet2.png)\n",
    "\n",
    "FC층은 output을 위한 용도 하나만 사용하며 conv는 모두 3 x 3을 사용합니다. 만약 50층 이상으로 깊어진다면 conv를 수행할 때, 밑의 그림처럼 앞서 GoogLeNet에서 사용한 bottleneck을 사용하여 연산을 간단하게 해줍니다.\n",
    "\n",
    "![resnet3](_image/resnet3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.6 DenseNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet은 모든 레이어의 feature map을 연결하는 방식의 신경망입니다. 이전 레이어의 feature map을 그 이후 모든 레이어의 feature map에 연결합니다. 연결할 때는 덧셈이 아닌 concatenate(연결)을 수행합니다. 따라서 연결하는 feature map의 크기는 동일해야 합니다. \n",
    "\n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F68j0G%2Fbtq0jOzmtVf%2FN9VjOkQErvexl7CvMS5ejK%2Fimg.png\">\n",
    "\n",
    "모든 레이어와 연결되었기에 기울기 소실 문제를 자연스레 해결할 수 있습니다. 또한 concatenate를 연결 연산으로 사용하기에 적은 depth(채널 수)를 사용합니다. 이는 파라미터수와 연산을 줄이게 됩니다. \n",
    "\n",
    "DenseNet 역시 연결 연산을 위해 크기가 같아야 하는데 pooling 연산은 크기를 감소시킵니다. 그렇기에 크기를 유지하면서 pooling을 수행할 수 있게하는 Dense Block을 사용합니다. Denseblock 사이에 pooling 연산을 수행하며 pooling 연산은 BN, 1 x 1 conv, 2 x 2 avg pool로 수행합니다. \n",
    "\n",
    "'Densely Connected Convolutional Networks'논문에서는 transition layer(pooling 연산)에 theta라는 하이퍼파라미터를 곱해줍니다. 이는 transition layer의 입력값 채널 수 m가 곱해져 theta x m 개의 채널수를 출력합니다. 논문에선 theta를 0.5로 설정하여 transition layer의 output의 채널 수를 0.5m으로 합니다. 즉, feature map의 크기와 채널 수를 감소시킵니다.\n",
    "\n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcRv7cz%2Fbtq0n9WUSCS%2FXHPMVeH4hP6jmv2UXnKdQk%2Fimg.png\">\n",
    "\n",
    "링크: https://deep-learning-study.tistory.com/528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습1. VGG Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습은 3 x 3 filter로 이루어진 VGG-19 Network를 직접 구현해보겠습니다. \n",
    "\n",
    "자세한 모델의 configuration은 아래 그림의 option E와 같습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1mFwqxP5rB4lhEfRk7OJla1wffKqEibQy)\n",
    "\n",
    "VGG-19는 2개의 layer로 구성된 2개의 convolution block과 4개의 layer로 구성된 3개의 convoultion block으로 나누어져 있습니다.\n",
    "\n",
    "또한, 학습 안전성을 위하여 각 layer는 convolution - batchnorm - relu로 이루어지며 매 block에 끝마다 2 x 2 maxpooling을 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tqdm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) VGG-19**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TwoLayerBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.output_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourLayerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FourLayerBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.output_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.output_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.output_dim, self.output_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        \n",
    "        self.block1 = TwoLayerBlock(3, 64) # [batch_size, 3, 32, 32] -> [batch_size, 64, 16, 16]\n",
    "        self.block2 = TwoLayerBlock(64, 128) # [batch_size, 64, 16, 16] -> [batch_size, 128, 8, 8]\n",
    "        self.block3 = FourLayerBlock(128, 256) # [batch_size, 128, 8, 8] -> [batch_size, 256, 4, 4]\n",
    "        self.block4 = FourLayerBlock(256, 512) # [batch_size, 256, 4, 4] -> [batch_size, 512, 2, 2]\n",
    "        self.block5 = FourLayerBlock(512, 512) # [batch_size, 512, 2, 2] -> [batch_size, 512, 1, 1]\n",
    "        \n",
    "        # squeeze로 1인 차원 제거 / [batch_size, 512, 1, 1] -> [batch_size, 512]\n",
    "        self.linear = nn.Sequential( # [batch_size, 512] -> [batch_size, 10]\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x) \n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = x.squeeze() # 차원이 1인 차원을 제거\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = torch.rand(64, 3, 32, 32)\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = VGG19()\n",
    "#print(model)\n",
    "\n",
    "output = model(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "if count_parameters(model) == 20365002:\n",
    "    print(\"success!\")\n",
    "else:\n",
    "    raise AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습2. Train CIFAR-10 with own VGG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train image: 50,000\n",
    "- test image: 10,000\n",
    "- class: [airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]\n",
    "\n",
    "data는 [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)에서 받아주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Use pre-defined dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTroch는 custom dataset과 dataloader를 사용해도 되지만 CIFAR-10과 같은 유명 데이터셋에 대해서는 pre-defined된 dataset이 존재합니다. \n",
    "\n",
    "이번 실습에선 custom dataset을 직접 만드는 대신 pre-trained dataset을 불러와서 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data/',      \n",
    "                                train=True, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data/',\n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, trainloader, testloader, model, optimizer, criterion, device):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        model: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        self.model.train()\n",
    "        loss_list = []\n",
    "        acc_list = []\n",
    "        for e in range(epoch):\n",
    "            running_loss, running_acc = 0.0, 0.0\n",
    "            for i, data in tqdm.tqdm(enumerate(self.trainloader, 0)): \n",
    "                inputs, labels = data \n",
    "                # model에 input으로 tensor를 gpu-device로 보낸다\n",
    "                inputs = inputs.to(self.device)  \n",
    "                labels = labels.to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()    \n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(inputs) \n",
    "                loss = self.criterion(outputs, labels)  \n",
    "                loss.backward() \n",
    "                self.optimizer.step() \n",
    "                running_loss += loss.item()\n",
    "                pred = outputs.max(1, keepdim=True)[1]\n",
    "                running_acc += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            \n",
    "            running_loss = running_loss / len(self.trainloader)\n",
    "            running_acc = running_acc / len(self.trainloader.dataset)\n",
    "            loss_list.append(running_loss)\n",
    "            acc_list.append(running_acc)\n",
    "            print('epoch: %d  loss: %.3f  acc:%.3f' % (e + 1, running_loss, running_acc))\n",
    "            \n",
    "        return loss_list, acc_list\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        test_acc = correct / len(self.testloader.dataset)\n",
    "        print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG19(\n",
       "  (block1): TwoLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block2): TwoLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block3): FourLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block4): FourLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block5): FourLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG19()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "device = 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [41:26,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.295  acc:0.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [41:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2  loss: 0.842  acc:0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [41:52,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3  loss: 0.636  acc:0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [44:36,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4  loss: 0.507  acc:0.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [26:35,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  loss: 0.405  acc:0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [27:31,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6  loss: 0.321  acc:0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [29:01,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7  loss: 0.255  acc:0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [31:17,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8  loss: 0.208  acc:0.930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [35:16,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9  loss: 0.169  acc:0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [29:24,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10  loss: 0.144  acc:0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_loader,\n",
    "                testloader = test_loader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "loss_list, acc_list = trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(x, loss_list)\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_title('loss_plot')\n",
    "\n",
    "axs[1].plot(x, acc_list)\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_title('acc_plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.816\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보면 test 정확도가 train 정확도에 비해 현저히 낮은 성능을 보입니다. 왜냐하면 overfitting이 일어나기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Train CIFAR-10 with pre-trained VGG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 위에서 만든 VGG Network를 통해 유의미한 정확도를 달성할 수 있습니다. 그러나 더 큰 모델을 사용할 경우, 매번 처음부터 학습을 진행하는 것은 쉬운 일이 아닙니다.\n",
    "\n",
    "따라서, 이번엔 pre-trained된 VGG model을 불러와서 linear layer만 추가적으로 학습시켜 적은 시간만으로 높은 성능을 달성하는 실습을 해보겠습니다.\n",
    "\n",
    "PyTorch에서 공식적으로 제공하는 pre-trained model은 [Torchvision.models](https://pytorch.org/vision/stable/models.html)에 있는 해당 document에서 찾아볼 수 있습니다. 또는 종종 google이나 facebook 등에서는 자신들이 개발한 모델에 대해서 pre-trained된 weight를 이와 같이 배포합니다. \n",
    "\n",
    "[예시](https://github.com/google-research/simclr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to C:\\Users\\delphinus/.cache\\torch\\hub\\checkpoints\\vgg19_bn-c79401a0.pth\n",
      "100%|██████████| 548M/548M [00:52<00:00, 10.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "pretrained_vgg = models.vgg19_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pretrained_VGG19(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(Pretrained_VGG19, self).__init__()\n",
    "        # inherit the weights from the pre-trained model\n",
    "        self.features = nn.Sequential(\n",
    "            *list(pretrained_model.features.children())\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze() \n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pretrained_VGG19(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pretrained_VGG19(pretrained_vgg)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "device = 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [27:39,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.742  acc:0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_loader,\n",
    "                testloader = test_loader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "loss_list, acc_list = trainer.train(epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.863\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Recurrent Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks(RNN)은 CNN과 다르게 이전 출력값을 다시 입력값으로 넣으며 사용하는 순환 신경망입니다. \n",
    "\n",
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" height = \"200px\" width = \"700px\">\n",
    "\n",
    "위 그림처럼 반복하는 것을 시간으로 나누어 나타낼 수 있습니다. 이러한 RNN의 은닉층 계산은 다음과 같이 진행됩니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22886/rnn_images4-5.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$h_t = tanh(W_x X_t + W_h h_{t-1} + b)$$\n",
    "\n",
    "$$y_t = W_y h_t$$\n",
    "\n",
    "이때 RNN은 zero-centered, 즉 평균이 0이여야 하기 때문에 tanh 함수를 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"hello\"라는 단어를 생성할 수 있도록 RNN을 학습시킨다고 가정할 때, 동작 과정은 다음과 같습니다.\n",
    "\n",
    "![hello](_image/hello.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Backpropagation throught Time (BPTT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN을 학습시킬 때 loss를 구하기 위해선 시간을 거슬러 올라가며 loss를 구해야 합니다. 이때 시간 방향으로 펼친 신경망의 Backpropagation이므로 이를 BPTT라고 합니다. \n",
    "\n",
    "![bptt](_image/bptt.png)\n",
    "\n",
    "BPTT를 이용하면 RNN을 학습할 수 있습니다. 그러나 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 메모리도 증가합니다. 또한 시간 크기가 커지면 역전파 시의 기울기가 불안정해지는 문제도 발생합니다. \n",
    "\n",
    "그렇기에 큰 시계열 데이터를 취급할 때는 신경망을 적당한 길이로 끊습니다. 이를 **Truncated BPTT** 라고 합니다. 이는 잘라낸 신경망끼리 독립적으로 backpropagation을 실행합니다. 물론 순전파의 연결은 끊으면 안되고 역전파의 연결만 끊는 것임을 기억해야 합니다. \n",
    "\n",
    "![truncated bptt](_image/truncated_bptt.png)\n",
    "\n",
    "BPTT는 RNN을 학습하는 방법이지만 기울기 소실이나 기울기 폭발 문제가 있습니다. BPTT는 $h_t= tanh(W_x X_t + W_h h_{t-1} + b)$를 역전파합니다. 덧셈은 기울기를 흘려보내기에 곰셉에 의해서 기울기는 전달됩니다. 이떄 곱셈의 역전파는 $W_h^T$가 계속해서 곱해집니다. 그렇기에 $W_h^T$가 1보다 크면 폭발하고 1보다 작으면 소실됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Gradient Clipping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 폭발 대책은 **기울기 클리핑(Gradient Clipping)** 을 사용합니다. \n",
    "\n",
    "$$if \\; \\lVert \\hat{g} \\rVert \\geq threshold \\; \\rightarrow \\; \\hat{g} = \\frac{threshold}{\\lVert \\hat{g} \\rVert} \\hat{g} $$\n",
    "\n",
    "threshold란 문턱값을 정하고 신경망에서 사용되는 모든 매개변수에 대한 기울기를 하나로 처리한다 가정하고 이를 $\\hat{g}$라고 썼습니다. 이때 기울기의 크기가 threshold를 넘어가면 오른쪽 항처럼 수정합니다. 간단하지만 많은 곳에서 잘 작동됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 LSTM(Long Short-Term Memory)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 소실 문제는 기울기 폭팔처럼 간단하게 해결되지 않습니다. 이를 해결하기 위해 RNN에 게이트를 추가하여 기울기 소실을 방지합니다. 먼저 게이트를 추가한 RNN 중 가장 대표적인 LSTM을 알아보겠습니다.\n",
    "\n",
    "<center><img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" height = \"300px\" width = \"700px\"></center>\n",
    "\n",
    "## <center> <**Standard RNN**> </center>\n",
    "\n",
    "<center><img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" height = \"300px\" width = \"700px\"></center>\n",
    "\n",
    "## <center><**LSTM**></center>\n",
    "\n",
    "위 두 그림은 일반적인 RNN과 LSTM을 나타냅니다. 내부 상황을 보지 않고 입출력만 봤을 때, LSTM은 입력이 하나 더 있음을 볼 수 있습니다. 그 입력은 출력되진 않으며 다음 layer로 넘어갑니다. 이를 $c$(memory cell, 기억셀)이라 하며, LSTM의 기억 매커니즘입니다. $c_t$는 시각 t에서의 LSTM의 기억이 저장되어있습니다. 이를 이용하여 기울기 소실을 방지합니다. 이제 자세한 동작을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Forget Gate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "Forget gate는 이전 레이어에서 넘어온 기억셀에서 불필요한 기억을 지워주는 게이트입니다. 위 $\\sigma$는 시그모이드 함수를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Input Gate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "입력된 정보들을 기억셀에새로 추가해야 합니다. 그러기 위해서 입력된 값에 tanh를 사용합니다. 이때, 모든 정보를 무비판적으로 다 수용하는 것이 아닌, 적절히 골라서 입력하도록 input gate가 동작합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) New Memory Cell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "forget gate와 input gate로 얻은 정보를 이용해 기억셀을 갱신합니다. 기억셀 중 한 줄기는 다음 레이어의 입력값으로 사용됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Output Gate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "Output gate는 먼저 얼마나 다음으로 흘려보낼지 $h_{t-1}$과 $x_t$로 결정합니다. 그리고 이 비율과 갱신한 기억셀을 이용해 output을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM이 기울기 소실을 방지하는 이유는 기억셀의 역전파에 주목해야 알 수 있습니다. 기억셀의 역전파는 곱셈과 덧셈만 있습니다. 덧셈은 기울기를 흘려보내고 곱셈은 원소별 곱으로 이루어져 있습니다. RNN은 역전파에서 계속 같은 가중치 행렬을 사용하여 행렬 곱을 사용했지만 원소별 곱셈을 사용하는 LSTM은 항상 곱해지는 값이 달라 기울기 소실을 방지할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5 GRU(Gated Recurrent Unit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "\n",
    "LSTM과 비슷한 생각으로 구현된 다른 방지 방법도 존재합니다. 그러나 GRU는 기억셀이 없습니다. 그리고 reset과 update라는 두 개의 게이트만 사용합니다. \n",
    "\n",
    "reset 게이트 $r$은 이전 은닉 상태를 얼마나 무시할지 결정합니다. 만약 $r$이 0이면 새로운 은닉 상태는 입력 $x_i$만으로 결정됩니다. 즉, 0이면 과거 상태는 완전히 무시하게 됩니다.\n",
    "\n",
    "update 게이트 $z$는 은닉 상태를 갱신하는 게이트입니다. LSTM의 forget 게이트, input 게이트가 담당하는 것을 혼자 담당하는 것입니다. $(1-z_t) * h_{t-1}$ 부분이 forget 게이트의 역할을 합니다. 이 계산으로 인해 과거의 은닉 상태에서 잊어야 할 정보를 삭제합니다. 그리고 $z * \\tilde{h}$ 부분이 input 게이트 기능을 하여 새로 추가된 정보에 가중치를 부여하게됩니다. \n",
    "\n",
    "이처럼 GRU는 LSTM을 더 단순하게 만든 것입니다. 따라서 LTSM보다 계산 비용을 줄이고 매개변수 개수도 줄일 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
